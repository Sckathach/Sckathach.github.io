---
title: "Exploring the multi-dimensional refusal subspace in reasoning models"
author: 
  - name: "Le magicien quantique"
toc: true
code-fold: true
date: 2025-09-15
bibliography: "references.bib"
reference-location: margin
---

## Evidences of multi-dimensionality

Mechanistic interpretability research (@nanda_comprehensive_2022 @hasting_introduction_2024) has shown that certain model behaviors are encoded linearly in the feature space (@park_linear_2024). For instance, by comparing activations between contrastive harmful-harmless sentence pairs, like "How to create a bomb?" and "How to create a website?", we can extract a refusal direction, a vector that represents the model's tendency to refuse harmful requests (@arditi_refusal_2024).

While earlier work assumed this refusal direction was one-dimensional (@arditi_refusal_2024), some evidence suggests the refusal behavior might actually spans a multi-dimensional subspace, especially in larger models (@wollschläger_geometry_2025 @winninger_using_2025). When we compute refusal directions using different methods (difference-in-means vs. probe classifiers), the resulting vectors have high but imperfect cosine similarity @fig-cosine.

![Cosine similarity between refusal directions extracted via different methods across layers of Llama 3.2 1b. While the cosine similarity is high in the first layers, it becomes less important in the later ones, with a minimal value of $0.39$ still being meaningful in $\mathbb{R}^{\text{d\_embed}}$, but far from the $1$ expected if refusal truly occupied just one dimension.](assets/cosine_pr_llama3.2_1b.svg){#fig-cosine}


This multi-dimensional structure helps explain why simply ablating a single refusal direction often fails to jailbreak larger models: the refusal behavior has redundancy across multiple dimensions. Previous work by @wollschläger_geometry_2025. characterized this as a refusal cone spanning multiple dimensions and proposed a gradient-based method to find it, though this approach is computationally expensive.

This may also explain why in SSR (@winninger_using_2025), the ProbeSSR method is significantly better than the SteeringSSR while being theoretically similar. The SteeringSSR uses difference in mean (DIM) to find one refusal direction in the cone, while the probes were initied at random and converged on a different, more effecient refusal direction.

Another element in favor of the multi-dimensional hypothesis is that in bigger models, ablating the refusal direction found with the DIM technique is not sufficient to induce jailbreak. The following section explore this argument in detail.

## Characterizing the refusal cone with a practical clusturing-based approach

While @wollschläger_geometry_2025 introduced a method to find the entire refusal cone with gradient descent, it is costly and cannot run on small setups. To enable experiments on bigger models, we developed a simpler method. The key insight is that, if models are large enough to encode different types of harmful content differently, we can extract diverse refusal vectors by using topically varied harmful-harmless pairs.

We combined several harmful datasets: AdvBench (@zou_universal_2023), HarmBench (@mazeika_harmbench_2024), StrongREJECT (@souly_strongreject_2024), ForbiddenQuestions (@chu_jailbreak_2025), MaliciousInstruct (@qiu_latent_2023), into a large Bigbench dataset, composed of 1200 samples. Using sentence transformers and HDBSCAN clustering, we grouped semantically similar harmful instructions together (@fig-clusters).

![Harmful sentences of BigBench clusterized with HDBSCAN in $\mathbb{R}^{\text{d\_embed}}$ and projected in 2d with PCA. Each cluster represent a distinct topic (e.g., "explosive creation", "hacking techniques", "illegal substances").](assets/clusters.svg){#fig-clusters}


By computing difference-in-means vectors from different topical subsets, we found multiple non-collinear refusal directions with approximately $0.30$ cosine similarity, which confirms they span different dimensions. This approach has two key advantages: it does not require any gradient descent method or hyperparameter to tune, and each direction can be interpreted semantically. However, it does not garantee that the whole refusal cone has been found.

## Experimental validation

To verify that these directions represent genuine components of the refusal subspace, we tested Qwen3-8b's resistance to vanilla harmful prompts while progressively ablating 1, 2, or 3 refusal directions. The ablated directions are not orthogonal in the sense of @wollschläger_geometry_2025, but it was still enough to provide meaningful results.

![Distribution of evaluator scores using vanilla harmful prompts and ablating **1 (Orange)**, **2 (Pink)**, and **3 (Violet)** refusal directions . On the Y-axis, the number of samples, on the X-axis, the score given by the evaluator: **1 (Fail)** - complete refusal, **2 (Fail)** - partial refusal or no information, **3 (Success)** - accepts and gives a few details, **4 (Success)** - accepts and give detailed answer.](assets/asr_multidim.svg){#fig-multi}


@fig-multi shows that, as more refusal directions are ablated, the distribution shifts rightward toward higher compliance scores. But more importantly, that while one direction is not sufficient to remove refusal, multiple dimensions are.

It is important to note that, compared to @wollschläger_geometry_2025, the ablated refusal direction may remove other capabilities of the model, as the refusal directions are computed without taking into account the success on harmless tasks.
