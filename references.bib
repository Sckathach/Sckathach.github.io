@article{liu_exploring_2024,
      title={Exploring Vulnerabilities and Protections in Large Language Models: A Survey}, 
      author={Frank Weizhen Liu and Chenhui Hu},
      year={2024},
      eprint={2406.00240},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@article{schulhoff_ignore_2024,
      title={Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition}, 
      author={Sander Schulhoff and Jeremy Pinto and Anaum Khan and Louis-François Bouchard and Chenglei Si and Svetlina Anati and Valen Tagliabue and Anson Liu Kost and Christopher Carnahan and Jordan Boyd-Graber},
      year={2024},
      eprint={2311.16119},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
}

@article{rumbelow_solidgoldmagikarp_2023,
    author = {Jessica Rumbelow and Mwatkins},
    title = {SolidGoldMagikarp (plus, prompt generation)},
    journal = {MATS Program},
    year = {2023},
}

@article{zou_poisonedrag_2024,
      title={PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models},
      author={Wei Zou and Runpeng Geng and Binghui Wang and Jinyuan Jia},
      year={2024},
      eprint={2402.07867},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
}

@article{anthropic_many_2024,
  title={Many-shot jailbreaking},
  author={Anthropic},
  year={2024}
}

@article{chaudhari_phantom_2024,
      title={Phantom: General Trigger Attacks on Retrieval Augmented Language Generation}, 
      author={Harsh Chaudhari and Giorgio Severi and John Abascal and Matthew Jagielski and Christopher A. Choquette-Choo and Milad Nasr and Cristina Nita-Rotaru and Alina Oprea},
      year={2024},
      eprint={2405.20485},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
}

@misc{nestaas_adversarial_2024,
	title = {Adversarial Search Engine Optimization for Large Language Models},
	abstract = {Large Language Models ({LLMs}) are increasingly used in applications where the model selects from competing third-party content, such as in {LLM}-powered search engines or chatbot plugins. In this paper, we introduce Preference Manipulation Attacks, a new class of attacks that manipulate an {LLM}’s selections to favor the attacker. We demonstrate that carefully crafted website content or plugin documentations can trick an {LLM} to promote the attacker products and discredit competitors, thereby increasing user traffic and monetization. We show this can lead to a prisoner’s dilemma, where all parties are incentivized to launch attacks, but this collectively degrades the {LLM}’s outputs for everyone. We demonstrate our attacks on production {LLM} search engines (Bing and Perplexity) and plugin {APIs} (for {GPT}-4 and Claude). As {LLMs} are increasingly used to rank third-party content, we expect Preference Manipulation Attacks to emerge as a significant threat.},
	number = {{arXiv}:2406.18382},
	publisher = {{arXiv}},
	author = {Nestaas, Fredrik and Debenedetti, Edoardo and Tramèr, Florian},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2406.18382 [cs]},
  year = {2024},
	keywords = {adv, llm, rag},
}

@article{chen_struq_2024,
	title = {{StruQ}: Defending Against Prompt Injection with Structured Queries},
	shorttitle = {{StruQ}},
	abstract = {Recent advances in Large Language Models ({LLMs}) enable exciting {LLM}-integrated applications, which perform textbased tasks by utilizing their advanced language understanding capabilities. However, as {LLMs} have improved, so have the attacks against them. Prompt injection attacks are an important threat: they trick the model to deviate from the original application’s instructions and instead follow user directives. These attacks rely on the {LLM}’s ability to follow instructions and inability to separate the prompts and user data. We introduce structured queries, a general approach to tackle this problem. Structured queries separate prompts and data into two channels. We implement a system that supports structured queries. This system is made of (1) a secure front-end that formats a prompt and user data into a special format, and (2) a specially trained {LLM} that can produce highquality outputs from these inputs. The {LLM} is trained using a novel fine-tuning strategy: we convert a base (non-instructiontuned) {LLM} to a structured instruction-tuned model that will only follow instructions in the prompt portion of a query. To do so, we augment standard instruction tuning datasets with examples that also include instructions in the data portion of the query, and fine-tune the model to ignore these. Our system significantly improves resistance to prompt injection attacks, with little or no impact on utility. Our code is released here.},
	number = {{arXiv}:2402.06363},
	publisher = {{arXiv}},
	author = {Chen, Sizhe and Piet, Julien and Sitawarin, Chawin and Wagner, David},
	langid = {english},
	eprinttype = {arxiv},
  year = {2024},
	eprint = {2402.06363 [cs]},
	keywords = {llm, def},
}

@article{wallace_instruction_2024,
      title={The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions}, 
      author={Eric Wallace and Kai Xiao and Reimar Leike and Lilian Weng and Johannes Heidecke and Alex Beutel},
      year={2024},
      eprint={2404.13208},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
}

@article{xiang_certifiable_2024,
      title={Certifiably Robust RAG against Retrieval Corruption}, 
      author={Chong Xiang and Tong Wu and Zexuan Zhong and David Wagner and Danqi Chen and Prateek Mittal},
      year={2024},
      eprint={2405.15556},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@article{piet_jatmo_2024,
      title={Jatmo: Prompt Injection Defense by Task-Specific Finetuning}, 
      author={Julien Piet and Maha Alrashed and Chawin Sitawarin and Sizhe Chen and Zeming Wei and Elizabeth Sun and Basel Alomair and David Wagner},
      year={2024},
      eprint={2312.17673},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
}

@book{amos_amortized_2023,
      title={Tutorial on amortized optimization}, 
      author={Brandon Amos},
      year={2023},
      eprint={2202.00665},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@article{zou_universal_2023,
      title={Universal and Transferable Adversarial Attacks on Aligned Language Models}, 
      author={Andy Zou and Zifan Wang and Nicholas Carlini and Milad Nasr and J. Zico Kolter and Matt Fredrikson},
      year={2023},
      eprint={2307.15043},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}



@article{paulus_advprompter_2024,
	title = {{AdvPrompter}: Fast Adaptive Adversarial Prompting for {LLMs}},
	shorttitle = {{AdvPrompter}},
	abstract = {While recently Large Language Models ({LLMs}) have achieved remarkable successes, they are vulnerable to certain jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires finding adversarial prompts that cause such jailbreaking, e.g. by appending a suffix to a given instruction, which is inefficient and time-consuming. On the other hand, automatic adversarial prompt generation often leads to semantically meaningless attacks that can easily be detected by perplexity-based filters, may require gradient information from the {TargetLLM}, or do not scale well due to time-consuming discrete optimization processes over the token space. In this paper, we present a novel method that uses another {LLM}, called the {AdvPrompter}, to generate human-readable adversarial prompts in seconds, \${\textbackslash}sim800{\textbackslash}times\$ faster than existing optimization-based approaches. We train the {AdvPrompter} using a novel algorithm that does not require access to the gradients of the {TargetLLM}. This process alternates between two steps: (1) generating high-quality target adversarial suffixes by optimizing the {AdvPrompter} predictions, and (2) low-rank fine-tuning of the {AdvPrompter} with the generated adversarial suffixes. The trained {AdvPrompter} generates suffixes that veil the input instruction without changing its meaning, such that the {TargetLLM} is lured to give a harmful response. Experimental results on popular open source {TargetLLMs} show state-of-the-art results on the {AdvBench} dataset, that also transfer to closed-source black-box {LLM} {APIs}. Further, we demonstrate that by fine-tuning on a synthetic dataset generated by {AdvPrompter}, {LLMs} can be made more robust against jailbreaking attacks while maintaining performance, i.e. high {MMLU} scores.},
	number = {{arXiv}:2404.16873},
	publisher = {{arXiv}},
	author = {Paulus, Anselm and Zharmagambetov, Arman and Guo, Chuan and Amos, Brandon and Tian, Yuandong},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2404.16873 [cs]},
  year = {2024},
	keywords = {adv, llm, prompt-hacking},
}

@article{jain_baseline_2023,
      title={Baseline Defenses for Adversarial Attacks Against Aligned Language Models}, 
      author={Neel Jain and Avi Schwarzschild and Yuxin Wen and Gowthami Somepalli and John Kirchenbauer and Ping-yeh Chiang and Micah Goldblum and Aniruddha Saha and Jonas Geiping and Tom Goldstein},
      year={2023},
      eprint={2309.00614},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@article{samvelyan_rainbow_2024,
      title={Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts}, 
      author={Mikayel Samvelyan and Sharath Chandra Raparthy and Andrei Lupu and Eric Hambro and Aram H. Markosyan and Manish Bhatt and Yuning Mao and Minqi Jiang and Jack Parker-Holder and Jakob Foerster and Tim Rocktäschel and Roberta Raileanu},
      year={2024},
      eprint={2402.16822},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@article{mouret_illuminating_2015,
      title={Illuminating search spaces by mapping elites}, 
      author={Jean-Baptiste Mouret and Jeff Clune},
      year={2015},
      eprint={1504.04909},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@article{wan_cyberseceval3_2024,
      title = {CyberSecEval 3: Advancing the Evaluation of Cybersecurity Risks and Capabilities in Large Language Models},
      author = {Shengye Wan, Cyrus Nikolaidis, Daniel Song, David Molnar, James Crnkovich, Jayson Grace, Manish Bhatt, Sahana Chennabasappa, Spencer Whitman, Stephanie Ding, Vlad Ionescu, Yue Li, Joshua Saxe},
      year = {2024},
      archivePrefix = {arXiv},
}

@article{inan_llamaguard_2024,
      title = {Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations},
      author = {Hakan Inan and Kartikeya Upasani and Jianfeng Chi and Rashi Rungta and Krithika Iyer and Yuning Mao and Davide Testuggine and Madian Khabsa},
      year = {2024},
      archivePrefix = {arXiv}
}

@article{sharma_spml_2024,
      title={SPML: A DSL for Defending Language Models Against Prompt Attacks}, 
      author={Reshabh K Sharma and Vinayak Gupta and Dan Grossman},
      year={2024},
      eprint={2402.11755},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@article{xiong_defensive_2024,
      title={Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks}, 
      author={Chen Xiong and Xiangyu Qi and Pin-Yu Chen and Tsung-Yi Ho},
      year={2024},
      eprint={2405.20099},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
}

@article{li_wmdp_2024,
      title={The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning}, 
      author={Nathaniel Li and Alexander Pan and Anjali Gopal and Summer Yue and Daniel Berrios and Alice Gatti and Justin D. Li and Ann-Kathrin Dombrowski and Shashwat Goel and Long Phan and Gabriel Mukobi and Nathan Helm-Burger and Rassin Lababidi and Lennart Justen and Andrew B. Liu and Michael Chen and Isabelle Barrass and Oliver Zhang and Xiaoyuan Zhu and Rishub Tamirisa and Bhrugu Bharathi and Adam Khoja and Zhenqi Zhao and Ariel Herbert-Voss and Cort B. Breuer and Samuel Marks and Oam Patel and Andy Zou and Mantas Mazeika and Zifan Wang and Palash Oswal and Weiran Lin and Adam A. Hunt and Justin Tienken-Harder and Kevin Y. Shih and Kemper Talley and John Guan and Russell Kaplan and Ian Steneker and David Campbell and Brad Jokubaitis and Alex Levinson and Jean Wang and William Qian and Kallol Krishna Karmakar and Steven Basart and Stephen Fitz and Mindy Levine and Ponnurangam Kumaraguru and Uday Tupakula and Vijay Varadharajan and Ruoyu Wang and Yan Shoshitaishvili and Jimmy Ba and Kevin M. Esvelt and Alexandr Wang and Dan Hendrycks},
      year={2024},
      eprint={2403.03218},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@article{conmy_automated_2023,
      title={Towards Automated Circuit Discovery for Mechanistic Interpretability}, 
      author={Arthur Conmy and Augustine N. Mavor-Parker and Aengus Lynch and Stefan Heimersheim and Adrià Garriga-Alonso},
      year={2023},
      eprint={2304.14997},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@article{turner_activation_2024,
      title={Activation Addition: Steering Language Models Without Optimization}, 
      author={Alexander Matt Turner and Lisa Thiergart and Gavin Leech and David Udell and Juan J. Vazquez and Ulisse Mini and Monte MacDiarmid},
      year={2024},
      eprint={2308.10248},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@article{mehrotra_tree_2023,
      title={Tree of Attacks: Jailbreaking Black-Box LLMs Automatically}, 
      author={Anay Mehrotra and Manolis Zampetakis and Paul Kassianik and Blaine Nelson and Hyrum Anderson and Yaron Singer and Amin Karbasi},
      year={2024},
      eprint={2312.02119},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@misc{yu_gptfuzzer_2024,
      title={GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts}, 
      author={Jiahao Yu and Xingwei Lin and Zheng Yu and Xinyu Xing},
      year={2024},
      eprint={2309.10253},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@article{zhao_weak_2024,
      title={Weak-to-Strong Jailbreaking on Large Language Models}, 
      author={Xuandong Zhao and Xianjun Yang and Tianyu Pang and Chao Du and Lei Li and Yu-Xiang Wang and William Yang Wang},
      year={2024},
      eprint={2401.17256},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.17256}, 
}

@article{li_llm_2024,
      title={LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet}, 
      author={Nathaniel Li and Ziwen Han and Ian Steneker and Willow Primack and Riley Goodside and Hugh Zhang and Zifan Wang and Cristina Menghini and Summer Yue},
      year={2024},
      eprint={2408.15221},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.15221}, 
}

@article{wang_hidden_2024,
      title={Hidden You Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Logic Chain Injection}, 
      author={Zhilong Wang and Yebo Cao and Peng Liu},
      year={2024},
      eprint={2404.04849},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2404.04849}, 
}

@article{yang_chain_2024,
      title={Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM}, 
      author={Xikang Yang and Xuehai Tang and Songlin Hu and Jizhong Han},
      year={2024},
      eprint={2405.05610},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.05610}, 
}

@article{russinovich_great_2024,
      title={Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack}, 
      author={Mark Russinovich and Ahmed Salem and Ronen Eldan},
      year={2024},
      eprint={2404.01833},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2404.01833}, 
}

@article{cheng_leveraging_2024,
      title={Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks}, 
      author={Yixin Cheng and Markos Georgopoulos and Volkan Cevher and Grigorios G. Chrysos},
      year={2024},
      eprint={2402.09177},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.09177}, 
}

@article{arditi_refusal_2024,
      title={Refusal in Language Models Is Mediated by a Single Direction}, 
      author={Andy Arditi and Oscar Obeso and Aaquib Syed and Daniel Paleka and Nina Panickssery and Wes Gurnee and Neel Nanda},
      year={2024},
      eprint={2406.11717},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.11717}, 
}

@article{sheshadri_latent_2024,
      title={Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs}, 
      author={Abhay Sheshadri and Aidan Ewart and Phillip Guo and Aengus Lynch and Cindy Wu and Vivek Hebbar and Henry Sleight and Asa Cooper Stickland and Ethan Perez and Dylan Hadfield-Menell and Stephen Casper},
      year={2024},
      eprint={2407.15549},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.15549}, 
}

@article{ilharco_editing_2023,
      title={Editing Models with Task Arithmetic}, 
      author={Gabriel Ilharco and Marco Tulio Ribeiro and Mitchell Wortsman and Suchin Gururangan and Ludwig Schmidt and Hannaneh Hajishirzi and Ali Farhadi},
      year={2023},
      eprint={2212.04089},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.04089}, 
}







@article{wortsman_modelsoups_2022,
      title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time}, 
      author={Mitchell Wortsman and Gabriel Ilharco and Samir Yitzhak Gadre and Rebecca Roelofs and Raphael Gontijo-Lopes and Ari S. Morcos and Hongseok Namkoong and Ali Farhadi and Yair Carmon and Simon Kornblith and Ludwig Schmidt},
      year={2022},
      eprint={2203.05482},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.05482}, 
}

@article{lucas_analyzing_2021,
      title={Analyzing Monotonic Linear Interpolation in Neural Network Loss Landscapes}, 
      author={James Lucas and Juhan Bae and Michael R. Zhang and Stanislav Fort and Richard Zemel and Roger Grosse},
      year={2021},
      eprint={2104.11044},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2104.11044}, 
}

@article{izmailov_averaging_2019,
      title={Averaging Weights Leads to Wider Optima and Better Generalization}, 
      author={Pavel Izmailov and Dmitrii Podoprikhin and Timur Garipov and Dmitry Vetrov and Andrew Gordon Wilson},
      year={2019},
      eprint={1803.05407},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1803.05407}, 
}

@article{ilharco_patching_2022,
      title={Patching open-vocabulary models by interpolating weights}, 
      author={Gabriel Ilharco and Mitchell Wortsman and Samir Yitzhak Gadre and Shuran Song and Hannaneh Hajishirzi and Simon Kornblith and Ali Farhadi and Ludwig Schmidt},
      year={2022},
      eprint={2208.05592},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2208.05592}, 
}



@article{olsson_incontext_2022,
      title={In-context Learning and Induction Heads}, 
      author={Catherine Olsson and Nelson Elhage and Neel Nanda and Nicholas Joseph and Nova DasSarma and Tom Henighan and Ben Mann and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and Danny Hernandez and Scott Johnston and Andy Jones and Jackson Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom Brown and Jack Clark and Jared Kaplan and Sam McCandlish and Chris Olah},
      year={2022},
      eprint={2209.11895},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.11895}, 
}

@article{power_grokking_2022,
      title={Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets}, 
      author={Alethea Power and Yuri Burda and Harri Edwards and Igor Babuschkin and Vedant Misra},
      year={2022},
      eprint={2201.02177},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2201.02177}, 
}

@article{pan_effect_2022,
      title={The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models}, 
      author={Alexander Pan and Kush Bhatia and Jacob Steinhardt},
      year={2022},
      eprint={2201.03544},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2201.03544}, 
}

@article{bricken_monosemanticity_2023,
     title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
     author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
     year={2023},
     journal={Transformer Circuits Thread},
     note={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
}


@article{chao_jailbreaking_2024,
      title={Jailbreaking Black Box Large Language Models in Twenty Queries}, 
      author={Patrick Chao and Alexander Robey and Edgar Dobriban and Hamed Hassani and George J. Pappas and Eric Wong},
      year={2024},
      eprint={2310.08419},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.08419}, 
}

@article{ebrahimi_hotflip_2018,
      title={HotFlip: White-Box Adversarial Examples for Text Classification}, 
      author={Javid Ebrahimi and Anyi Rao and Daniel Lowd and Dejing Dou},
      year={2018},
      eprint={1712.06751},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1712.06751}, 
}

@article{cornacchia_moje_2024,
      title={MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks}, 
      author={Giandomenico Cornacchia and Giulio Zizzo and Kieran Fraser and Muhammad Zaid Hameed and Ambrish Rawat and Mark Purcell},
      year={2024},
      eprint={2409.17699},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2409.17699}, 
}

@article{garak,
  title={{garak: A Framework for Security Probing Large Language Models}},
  author={Leon Derczynski and Erick Galinkin and Jeffrey Martin and Subho Majumdar and Nanna Inie},
  year={2024},
  howpublished={\url{https://garak.ai}}
}


@article{nasr_scalable_2023,
      title={Scalable Extraction of Training Data from (Production) Language Models}, 
      author={Milad Nasr and Nicholas Carlini and Jonathan Hayase and Matthew Jagielski and A. Feder Cooper and Daphne Ippolito and Christopher A. Choquette-Choo and Eric Wallace and Florian Tramèr and Katherine Lee},
      year={2023},
      eprint={2311.17035},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.17035}, 
}


@article{huang_endless_2024,
  title={Endless Jailbreaks with Bijection Learning},
  author={Huang, Brian R.Y. and Li, Maximilian and Tang, Leonard},
  journal={arXiv preprint arXiv:2410.01294},
  year={2024},
  note={Haize Labs}
}

@misc{olah_distributed_2023,
      title={Distributed Representations: Composition & Superposition},
      author={Chris Olah},
      jounal={Transformer Circuit Thread},
      year={2023},
}

@article{cunningham_sparse_2023,
      title={Sparse Autoencoders Find Highly Interpretable Features in Language Models}, 
      author={Hoagy Cunningham and Aidan Ewart and Logan Riggs and Robert Huben and Lee Sharkey},
      year={2023},
      eprint={2309.08600},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.08600}, 
}


@misc{openai_gpt4o_2024,
      title={OpenAI o1 System Card},
      author={OpenAI},
      year={2024},
      url={https://assets.ctfassets.net/kftzwdyauwt9/67qJD51Aur3eIc96iOfeOP/71551c3d223cd97e591aa89567306912/o1_system_card.pdf}
}

@article{openai_language_2023,
      title={Language models can explain neurons in language models},
      author={OpenAI},
      year={2023},
      url={https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html}
}


@article{liu_autodan_2024,
      title={AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs}, 
      author={Xiaogeng Liu and Peiran Li and Edward Suh and Yevgeniy Vorobeychik and Zhuoqing Mao and Somesh Jha and Patrick McDaniel and Huan Sun and Bo Li and Chaowei Xiao},
      year={2024},
      eprint={2410.05295},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2410.05295}, 
}

@article{mo_fight_2024,
      title={Fight Back Against Jailbreaking via Prompt Adversarial Tuning}, 
      author={Yichuan Mo and Yuji Wang and Zeming Wei and Yisen Wang},
      year={2024},
      eprint={2402.06255},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.06255}, 
}

@article{guth_rainbow_2023,
  title={A Rainbow in Deep Network Black Boxes},
  author={Guth, Florentin and M{\'e}nard, Brice and Rochette, Gaspar and Mallat, St{\'e}phane},
  journal={arXiv preprint arXiv:2305.18512},
  year={2023}
}

@article{wei_jailbroken_2023,
      title={Jailbroken: How Does LLM Safety Training Fail?}, 
      author={Alexander Wei and Nika Haghtalab and Jacob Steinhardt},
      year={2023},
      eprint={2307.02483},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.02483}, 
}

@article{ball_understanding_2024,
      title={Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models}, 
      author={Sarah Ball and Frauke Kreuter and Nina Panickssery},
      year={2024},
      eprint={2406.09289},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.09289}, 
}

@article{lee_mechanistic_2024,
      title={A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity}, 
      author={Andrew Lee and Xiaoyan Bai and Itamar Pres and Martin Wattenberg and Jonathan K. Kummerfeld and Rada Mihalcea},
      year={2024},
      eprint={2401.01967},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.01967}, 
}

@article{guo_mechanistic_2024,
      title={Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization}, 
      author={Phillip Guo and Aaquib Syed and Abhay Sheshadri and Aidan Ewart and Gintare Karolina Dziugaite},
      year={2024},
      eprint={2410.12949},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.12949}, 
}

@article{xhonneux_incontext_2024,
      title={In-Context Learning Can Re-learn Forbidden Tasks}, 
      author={Sophie Xhonneux and David Dobre and Jian Tang and Gauthier Gidel and Dhanya Sridhar},
      year={2024},
      eprint={2402.05723},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.05723}, 
}

@article{mordvintsev_inceptionism_2015,
  author = {Alexander Mordvintsev, Christopher Olah, Mike Tyka},
  year = {2015},
  title = {Inceptionism: Going Deeper into Neural Networks},
  journal = {Google Research Blog}
}

@article{hasting_introduction_2024,
  author = {Sarah Hastings-Woodhouse},
  title = {Introduction to Mechanistic Interpretability},
  year = {2024},
  url = {https://aisafetyfundamentals.com/blog/introduction-to-mechanistic-interpretability/}
}

@article{lambert_illustrating_2022,
  author = {Lambert, Nathan and Castricato, Louis and von Werra, Leandro and Havrilla, Alex},
  title = {Illustrating Reinforcement Learning from Human Feedback (RLHF)},
  journal = {Hugging Face Blog},
  year = {2022},
  note = {https://huggingface.co/blog/rlhf},
}

@article{bai_constitutional_2022,
	title = {Constitutional {AI}: Harmlessness from {AI} Feedback},
	shorttitle = {Constitutional {AI}},
	abstract = {As {AI} systems become more capable, we would like to enlist their help to supervise other {AIs}. We experiment with methods for training a harmless {AI} assistant through selfimprovement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as ‘Constitutional {AI}’. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then ﬁnetune the original model on revised responses. In the {RL} phase, we sample from the ﬁnetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of {AI} preferences. We then train with {RL} using the preference model as the reward signal, i.e. we use ‘{RL} from {AI} Feedback’ ({RLAIF}). As a result we are able to train a harmless but nonevasive {AI} assistant that engages with harmful queries by explaining its objections to them. Both the {SL} and {RL} methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of {AI} decision making. These methods make it possible to control {AI} behavior more precisely and with far fewer human labels.},
	number = {{arXiv}:2212.08073},
	publisher = {{arXiv}},
	author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and {McKinnon}, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and Tran-Johnson, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and {DasSarma}, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and Telleen-Lawton, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R. and Hatfield-Dodds, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and {McCandlish}, Sam and Brown, Tom and Kaplan, Jared},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2212.08073 [cs]},
	keywords = {llm, rlhf, safety},
}


@article{casper_openproblems_2023,
      title={Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback}, 
      author={Stephen Casper and Xander Davies and Claudia Shi and Thomas Krendl Gilbert and Jérémy Scheurer and Javier Rando and Rachel Freedman and Tomasz Korbak and David Lindner and Pedro Freire and Tony Wang and Samuel Marks and Charbel-Raphaël Segerie and Micah Carroll and Andi Peng and Phillip Christoffersen and Mehul Damani and Stewart Slocum and Usman Anwar and Anand Siththaranjan and Max Nadeau and Eric J. Michaud and Jacob Pfau and Dmitrii Krasheninnikov and Xin Chen and Lauro Langosco and Peter Hase and Erdem Bıyık and Anca Dragan and David Krueger and Dorsa Sadigh and Dylan Hadfield-Menell},
      year={2023},
      eprint={2307.15217},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2307.15217}, 
}

@article{elhage_superposition_2022,
   title={Toy Models of Superposition},
   author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/toy_model/index.html}
}

@article{olah_zoom_2020,
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {Zoom In: An Introduction to Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/zoom-in},
  doi = {10.23915/distill.00024.001}
}

@article{olah_overview_2020,
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {An Overview of Early Vision in InceptionV1},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/early-vision},
  doi = {10.23915/distill.00024.002}
}

@article{jiang_discrete_2019,
	author = {Jiang, Rundong and Li, Ming and Tang, Shiming},
	title = {Discrete neural clusters encode orientation, curvature and corners in macaque V4},
	year = {2019},
	doi = {10.1101/808907},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {As an intermediate stage of the ventral visual pathway, V4 plays a crucial role in transforming basic orientation information into higher-ordered object representations. However, the neural mechanisms underlying the encoding of simple, complex and intermediate features in V4 remain poorly understood. Using two-photon calcium imaging in awake macaques, we recorded the responses of large populations of V4 neurons to thousands of natural images. To understand these high-dimensional data sets, we performed dimensional reduction analysis on the neuronal population responses. We found orthogonal dimensions encoding orientation and more complex features, with curves and corners represented separately. These distinct feature dimensions were encoded by spatially clustered subpopulations of neurons organized in iso-orientation and curvature domains. Using synthetic stimuli, we confirmed that curvature and corner selective neurons in V4 were tuned to integral features rather than combinations of local orientation compartments. Our results show that curves and corners are encoded by neurons clustered into functional domains separate from orientation. This functionally specific population architecture may serve to facilitate local computations that expedite more complex shape and object representations at higher levels of the ventral pathway.},
	journal = {bioRxiv}
}

@article{nanda_comprehensive_2022, 
  title={A Comprehensive Mechanistic Interpretability Explainer & Glossary}, 
  url={https://neelnanda.io/glossary}, 
  author={Nanda, Neel}, 
  year={2022}, 
  month={Dec}
}

@article{mikolov_efficient_2013,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1301.3781}, 
}

@article{socher_zeroshot_2013,
      title={Zero-Shot Learning Through Cross-Modal Transfer}, 
      author={Richard Socher and Milind Ganjoo and Hamsa Sridhar and Osbert Bastani and Christopher D. Manning and Andrew Y. Ng},
      year={2013},
      eprint={1301.3666},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1301.3666}, 
}



@article{zou_representation_2023,
      title={Representation Engineering: A Top-Down Approach to AI Transparency}, 
      author={Andy Zou and Long Phan and Sarah Chen and James Campbell and Phillip Guo and Richard Ren and Alexander Pan and Xuwang Yin and Mantas Mazeika and Ann-Kathrin Dombrowski and Shashwat Goel and Nathaniel Li and Michael J. Byun and Zifan Wang and Alex Mallen and Steven Basart and Sanmi Koyejo and Dawn Song and Matt Fredrikson and J. Zico Kolter and Dan Hendrycks},
      year={2023},
      eprint={2310.01405},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.01405}, 
}


@article{nanda_transformerlens_2022,
    title = {TransformerLens},
    author = {Neel Nanda and Joseph Bloom},
    year = {2022},
    howpublished = {\url{https://github.com/TransformerLensOrg/TransformerLens}},
}

@article{bloom_sae_2024,
   title = {SAELens},
   author = {Joseph Bloom and David Chanin},
   year = {2024},
   howpublished = {\url{https://github.com/jbloomAus/SAELens}},
}


@article{bereska_mechanistic_2024,
      title={Mechanistic Interpretability for AI Safety -- A Review}, 
      author={Leonard Bereska and Efstratios Gavves},
      year={2024},
      eprint={2404.14082},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2404.14082}, 
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@article{hayase_query_2024,
      title={Query-Based Adversarial Prompt Generation}, 
      author={Jonathan Hayase and Ema Borevkovic and Nicholas Carlini and Florian Tramèr and Milad Nasr},
      year={2024},
      eprint={2402.12329},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.12329}, 
}

@article{ebrahimi_hotflip_2017,
      title = {HotFlip: White-Box Adversarial Examples for Text Classification},
      author={Javid Ebrahimi, Anyi Rao, Daniel Lowd, Dejing Dou},
      year={2017},
      eprint={1712.06751},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1712.06751},
}


@misc{haize_sota_2024,
      title={Making a SOTA Adversarial Attack on LLMs 38x Faster},
      author={Haize},
      year={2024},
      url={https://blog.haizelabs.com/posts/acg/}
}

@article{peng_interpreting_2024,
      title={Interpreting the Curse of Dimensionality from Distance Concentration and Manifold Effect}, 
      author={Dehua Peng and Zhipeng Gui and Huayi Wu},
      year={2024},
      eprint={2401.00422},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.00422}, 
}
@article{he_jailbreaklens_2024,
      title={JailbreakLens: Interpreting Jailbreak Mechanism in the Lens of Representation and Circuit}, 
      author={Zeqing He and Zhibo Wang and Zhixuan Chu and Huiyu Xu and Rui Zheng and Kui Ren and Chun Chen},
      year={2024},
      eprint={2411.11114},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2411.11114}, 
}


@misc{wang_attngcg_2024,
      title={AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation}, 
      author={Zijun Wang and Haoqin Tu and Jieru Mei and Bingchen Zhao and Yisen Wang and Cihang Xie},
      year={2024},
      eprint={2410.09040},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.09040}, 
}

@article{liao_amplegcg_2024,
  title={AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs},
  author={Liao, Zeyi and Sun, Huan},
  journal={arXiv preprint arXiv:2404.07921},
  year={2024}
}

@article{kumar_amplegcg_2024,
  title={AmpleGCG-Plus: A Strong Generative Model of Adversarial Suffixes to Jailbreak LLMs with Higher Success Rates in Fewer Attempts},
  author={Kumar, Vishal and Liao, Zeyi and Jones, Jaylen and Sun, Huan},
  journal={arXiv preprint arXiv:2410.22143},
  year={2024}
}

@misc{jia_improved_2024,
      title={Improved Techniques for Optimization-Based Jailbreaking on Large Language Models}, 
      author={Xiaojun Jia and Tianyu Pang and Chao Du and Yihao Huang and Jindong Gu and Yang Liu and Xiaochun Cao and Min Lin},
      year={2024},
      eprint={2405.21018},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.21018}, 
}

@misc{ball_understanding_2024,
      title={Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models}, 
      author={Sarah Ball and Frauke Kreuter and Nina Panickssery},
      year={2024},
      eprint={2406.09289},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.09289}, 
}